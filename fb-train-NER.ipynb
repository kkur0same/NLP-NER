{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp model.py /root/miniconda3/lib/python3.8/site-packages/tez/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -cvf 6664-raw-dice-07gm-8f.tar /root/autodl-tmp/6664-raw-dice-07gm-8f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gzip /root/autodl-tmp/6664-raw-dice-07gm-8f.tar\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkirob\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from tez import enums\n",
    "from tez.callbacks import Callback\n",
    "from tqdm import tqdm as tqdm1\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "#     from kaggle_secrets import UserSecretsClient\n",
    "#     user_secrets = UserSecretsClient()\n",
    "#     api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key='d6e9e960dbfb433f958557d6d24fe8d5f4e05c49')\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"./fb-train.ipynb\"\n",
    "\n",
    "\n",
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}\n",
    "\n",
    "\n",
    "def _prepare_training_data_helper(args, tokenizer, df, train_ids):\n",
    "    training_samples = []\n",
    "    for idx in tqdm1(train_ids):\n",
    "        filename = os.path.join(args.input, \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        temp_df = df[df[\"id\"] == idx]\n",
    "        for _, row in temp_df.iterrows():\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, args, num_jobs):\n",
    "    training_samples = []\n",
    "    train_ids = df[\"id\"].unique()\n",
    "\n",
    "    train_ids_splits = np.array_split(train_ids, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(args, tokenizer, df, idx) for idx in train_ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "#     joblib.dump(training_samples, 'training_samples.pkl')\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(\" \"))\n",
    "    set_gt = set(row.predictionstring_gt.split(\" \"))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter / len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp_micro(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    This code is from Rob Mulla's Kaggle kernel.\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    pred_df[\"pred_id\"] = pred_df.index\n",
    "    gt_df[\"gt_id\"] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(\n",
    "        gt_df,\n",
    "        left_on=[\"id\", \"class\"],\n",
    "        right_on=[\"id\", \"discourse_type\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_pred\", \"_gt\"),\n",
    "    )\n",
    "    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n",
    "    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n",
    "\n",
    "    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n",
    "    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n",
    "    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n",
    "    tp_pred_ids = (\n",
    "        joined.query(\"potential_TP\")\n",
    "        .sort_values(\"max_overlap\", ascending=False)\n",
    "        .groupby([\"id\", \"predictionstring_gt\"])\n",
    "        .first()[\"pred_id\"]\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n",
    "    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    # calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n",
    "    return my_f1_score\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n",
    "        pred_subset = pred_df.loc[pred_df[\"class\"] == discourse_type].reset_index(drop=True).copy()\n",
    "        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1\n",
    "\n",
    "\n",
    "class FeedbackDatasetValid:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"ids\": input_ids,\n",
    "            \"mask\": attention_mask,\n",
    "        }\n",
    "class EvaluationScheduler:\n",
    "    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n",
    "        self.evaluation_schedule = evaluation_schedule\n",
    "        self.evaluation_interval = self.evaluation_schedule[0][1]\n",
    "        self.last_evaluation_step = 0\n",
    "        self.prev_loss = float('inf')\n",
    "        self.penalize_factor = penalize_factor\n",
    "        self.penalty = 0\n",
    "        self.prev_interval = -1\n",
    "        self.max_penalty = max_penalty\n",
    "\n",
    "    def step(self, step):\n",
    "        # should we to make evaluation right now\n",
    "        if step >= self.last_evaluation_step + self.evaluation_interval:\n",
    "            self.last_evaluation_step = step\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "            \n",
    "    def update_evaluation_interval(self, last_loss):\n",
    "        # set up evaluation_interval depending on loss value\n",
    "        cur_interval = -1\n",
    "        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n",
    "            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n",
    "                self.evaluation_interval = interval\n",
    "                cur_interval = i\n",
    "                break\n",
    "        if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n",
    "            self.penalty += self.penalize_factor\n",
    "            self.penalty = min(self.penalty, self.max_penalty)\n",
    "            self.evaluation_interval += self.penalty\n",
    "        else:\n",
    "            self.penalty = 0\n",
    "            \n",
    "        self.prev_loss = last_loss\n",
    "        self.prev_interval = cur_interval\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def jn(pst, start, end):\n",
    "#     return \" \".join([str(x) for x in pst[start:end]])\n",
    "\n",
    "\n",
    "# def link_evidence(oof):\n",
    "#     thresh = 1\n",
    "#     idu = oof['id'].unique()\n",
    "#     idc = idu[1]\n",
    "#     eoof = oof[oof['class'] == \"Evidence\"]\n",
    "#     neoof = oof[oof['class'] != \"Evidence\"]\n",
    "#     for thresh2 in range(26,27, 1):\n",
    "#         retval = []\n",
    "#         for idv in idu:\n",
    "#             for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "#                    'Counterclaim', 'Rebuttal']:\n",
    "#                 q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "#                 if len(q) == 0:\n",
    "#                     continue\n",
    "#                 pst = []\n",
    "#                 for i,r in q.iterrows():\n",
    "#                     pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "#                 start = 1\n",
    "#                 end = 1\n",
    "#                 for i in range(2,len(pst)):\n",
    "#                     cur = pst[i]\n",
    "#                     end = i\n",
    "#                     #if pst[start] == 205:\n",
    "#                     #   print(cur, pst[start], cur - pst[start])\n",
    "#                     if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "#                         retval.append((idv, c, jn(pst, start, end)))\n",
    "#                         start = i + 1\n",
    "#                 v = (idv, c, jn(pst, start, end+1))\n",
    "#                 #print(v)\n",
    "#                 retval.append(v)\n",
    "#         roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "#         roof = roof.merge(neoof, how='outer')\n",
    "#         return roof\n",
    "def link_evidence(oof):\n",
    "    if not len(oof):\n",
    "        return oof\n",
    "  \n",
    "    def jn(pst, start, end):\n",
    "        return \" \".join([str(x) for x in pst[start:end]])\n",
    "  \n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    eoof.index = eoof[['id', 'class']]\n",
    "    for thresh2 in range(26, 27, 1):\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in ['Evidence']:\n",
    "                q = eoof[(eoof['id'] == idv)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for r in q.itertuples():\n",
    "                    pst = [*pst, -1,  *[int(x) for x in r.predictionstring.split()]]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2, len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    if  ((cur == -1) and ((pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end + 1))\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof\n",
    "class Valid_schedule(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        valid_df,\n",
    "        valid_samples,\n",
    "        batch_size,\n",
    "        tokenizer,\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        delta=0.001,\n",
    "        save_weights_only=True,\n",
    "        evaluation_scheduler = None,\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.epoch_score = None\n",
    "        self.early_stop = False\n",
    "        self.step = 0\n",
    "        self.delta = delta\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.model_path = model_path\n",
    "        self.valid_samples = valid_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_df = valid_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.evaluation_scheduler = evaluation_scheduler\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "#     def on_train_step_end(self,model):\n",
    "#         if self.evaluation_scheduler.step(self.step):\n",
    "#             self.validate(model)\n",
    "#         self.step += 1\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        self.validate(model)\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * self.epoch_score\n",
    "        else:\n",
    "            score = np.copy(self.epoch_score)\n",
    "        if score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopping counter: {} out of {}\".format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                model.model_state = enums.ModelState.END\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\"Validation score improved ({} --> {}). Saving model!\".format(self.val_score, epoch_score))\n",
    "            model.save(self.model_path, weights_only=self.save_weights_only)\n",
    "        self.val_score = epoch_score\n",
    "        \n",
    "    def validate(self, model):\n",
    "        model.eval()\n",
    "        valid_dataset = FeedbackDatasetValid(self.valid_samples, 4096, self.tokenizer)\n",
    "        collate = Collate(self.tokenizer)\n",
    "\n",
    "        preds_iter = model.predict(\n",
    "            valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            n_jobs=-1,\n",
    "            collate_fn=collate,\n",
    "        )\n",
    "\n",
    "        final_preds = []\n",
    "        final_scores = []\n",
    "        for preds in preds_iter:\n",
    "            pred_class = np.argmax(preds, axis=2)\n",
    "            pred_scrs = np.max(preds, axis=2)\n",
    "            for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "                final_preds.append(pred.tolist())\n",
    "                final_scores.append(pred_scr.tolist())\n",
    "\n",
    "        for j in range(len(self.valid_samples)):\n",
    "            tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "            tt_score = final_scores[j][1:]\n",
    "            self.valid_samples[j][\"preds\"] = tt\n",
    "            self.valid_samples[j][\"pred_scores\"] = tt_score\n",
    "\n",
    "        submission = []\n",
    "        min_thresh = {\n",
    "            \"Lead\": 9,\n",
    "            \"Position\": 5,\n",
    "            \"Evidence\": 14,\n",
    "            \"Claim\": 3,\n",
    "            \"Concluding Statement\": 11,\n",
    "            \"Counterclaim\": 6,\n",
    "            \"Rebuttal\": 4,\n",
    "        }\n",
    "        proba_thresh = {\n",
    "            \"Lead\": 0.7,\n",
    "            \"Position\": 0.55,\n",
    "            \"Evidence\": 0.65,\n",
    "            \"Claim\": 0.55,\n",
    "            \"Concluding Statement\": 0.7,\n",
    "            \"Counterclaim\": 0.5,\n",
    "            \"Rebuttal\": 0.55,\n",
    "        }\n",
    "\n",
    "        for _, sample in enumerate(self.valid_samples):\n",
    "            preds = sample[\"preds\"]\n",
    "            offset_mapping = sample[\"offset_mapping\"]\n",
    "            sample_id = sample[\"id\"]\n",
    "            sample_text = sample[\"text\"]\n",
    "            sample_pred_scores = sample[\"pred_scores\"]\n",
    "\n",
    "            # pad preds to same length as offset_mapping\n",
    "            if len(preds) < len(offset_mapping):\n",
    "                preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n",
    "                sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n",
    "\n",
    "            idx = 0\n",
    "            phrase_preds = []\n",
    "            while idx < len(offset_mapping):\n",
    "                start, _ = offset_mapping[idx]\n",
    "                if preds[idx] != \"O\":\n",
    "                    label = preds[idx][2:]\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                phrase_scores = []\n",
    "                phrase_scores.append(sample_pred_scores[idx])\n",
    "                idx += 1\n",
    "                while idx < len(offset_mapping):\n",
    "                    if label == \"O\":\n",
    "                        matching_label = \"O\"\n",
    "                    else:\n",
    "                        matching_label = f\"I-{label}\"\n",
    "                    if preds[idx] == matching_label:\n",
    "                        _, end = offset_mapping[idx]\n",
    "                        phrase_scores.append(sample_pred_scores[idx])\n",
    "                        idx += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if \"end\" in locals():\n",
    "                    phrase = sample_text[start:end]\n",
    "                    phrase_preds.append((phrase, start, end, label, phrase_scores))\n",
    "\n",
    "            temp_df = []\n",
    "            for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "                word_start = len(sample_text[:start].split())\n",
    "                word_end = word_start + len(sample_text[start:end].split())\n",
    "                word_end = min(word_end, len(sample_text.split()))\n",
    "                ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "                if label != \"O\":\n",
    "                    if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "                        if len(ps.split()) >= min_thresh[label]:\n",
    "                            temp_df.append((sample_id, label, ps))\n",
    "\n",
    "            temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "\n",
    "            submission.append(temp_df)\n",
    "\n",
    "        submission = pd.concat(submission).reset_index(drop=True)\n",
    "        submission = link_evidence(submission)\n",
    "#         submission[\"len\"] = submission.predictionstring.apply(lambda x: len(x.split()))\n",
    "\n",
    "#         def threshold(df):\n",
    "#             df = df.copy()\n",
    "#             for key, value in min_thresh.items():\n",
    "#                 index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n",
    "#                 df.drop(index, inplace=True)\n",
    "#             return df\n",
    "\n",
    "#         submission = threshold(submission)\n",
    "\n",
    "#         # drop len\n",
    "#         submission = submission.drop(columns=[\"len\"])\n",
    "\n",
    "        scr = score_feedback_comp(submission, self.valid_df, return_class_scores=True)\n",
    "        print(scr)\n",
    "        model.train()\n",
    "        score = scr[0]\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            wandb.log({\"Best accuracy\": self.best_score})\n",
    "            self.save_checkpoint(score, model)\n",
    "        if self.best_score is not None:\n",
    "            if score >= self.best_score:\n",
    "                self.best_score = score\n",
    "                wandb.log({\"Best accuracy\": self.best_score})\n",
    "                self.save_checkpoint(score, model)\n",
    "                self.counter = 0\n",
    "        self.epoch_score = score\n",
    "        self.evaluation_scheduler.update_evaluation_interval(1.0-score)\n",
    "        wandb.log({\"Valid accuracy\":score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup,DebertaV2TokenizerFast\n",
    "from  dice_loss.loss import  DiceLoss\n",
    "from ChildTuningAdamW import ChildTuningAdamW\n",
    "\n",
    "# from utils import EarlyStopping, prepare_training_data, target_id_map\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--fold\", type=int, required=True)\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    parser.add_argument(\"--lr\", type=float, required=True)\n",
    "    parser.add_argument(\"--output\", type=str, default=\"../model\", required=False)\n",
    "    parser.add_argument(\"--input\", type=str, default=\"../input\", required=False)\n",
    "    parser.add_argument(\"--max_len\", type=int, default=1024, required=False)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, required=False)\n",
    "    parser.add_argument(\"--valid_batch_size\", type=int, default=8, required=False)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=20, required=False)\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=1, required=False)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "class FeedbackDataset:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_labels = self.samples[idx][\"input_labels\"]\n",
    "        input_labels = [target_id_map[x] for x in input_labels]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "        # print(input_ids)\n",
    "        # print(input_labels)\n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            if self.tokenizer.padding_side == \"right\":\n",
    "                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                input_labels = input_labels + [padding_label_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            else:\n",
    "                input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "                input_labels = [padding_label_id] * padding_length + input_labels\n",
    "                attention_mask = [0] * padding_length + attention_mask\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(input_labels, dtype=torch.long),\n",
    "        }\n",
    "class WeightedLayerPooling(tez.Model):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1,2,3,4], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "    \n",
    "class FeedbackModel(tez.Model):\n",
    "    def __init__(self, model_name, num_train_steps, learning_rate, num_labels, steps_per_epoch):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_labels = num_labels\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.step_scheduler_after = \"batch\"\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        # self.pooling = WeightedLayerPooling(config.num_hidden_layers,\n",
    "        #                                     layer_start=21,\n",
    "        #                                     layer_weights=None)\n",
    "        # self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"num_labels\": self.num_labels,\n",
    "                # \"num_hidden_layers\": 25,\n",
    "                # \"attention_window\":[512]*25,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        # gradient_mask = np.load('./autodl-tmp/longformer_0.7_gm.npy',allow_pickle=True).item()\n",
    "        # opt = ChildTuningAdamW(optimizer_parameters,reserve_p = 0.7,mode = 'ChildTuning-D',lr = self.learning_rate)\n",
    "        # opt.set_gradient_mask(gradient_mask)\n",
    "        opt = AdamW(optimizer_parameters, lr=self.learning_rate)\n",
    "        return opt\n",
    "#     def fetch_optimizer(self):\n",
    "#         named_parameters = list(model.named_parameters())    \n",
    "#         parameters = []\n",
    "\n",
    "#         # increase lr every second layer\n",
    "#         increase_lr_every_k_layer = 1\n",
    "#         lrs = np.linspace(1, 5, 48 // increase_lr_every_k_layer)\n",
    "#         num = 0\n",
    "#         for layer_num, (name, params) in enumerate(named_parameters):\n",
    "#             weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "#             splitted_name = name.split('.')\n",
    "#             lr = self.learning_rate\n",
    "#             if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "#                 layer_num = int(splitted_name[3])\n",
    "#                 lr = lrs[layer_num // increase_lr_every_k_layer] * self.learning_rate\n",
    "#                 # num+=1\n",
    "#             if splitted_name[0] in ['output']:\n",
    "#                 lr = 5 * self.learning_rate\n",
    "#             # print(num)\n",
    "#             parameters.append({\"params\": params,\n",
    "#                                \"weight_decay\": weight_decay,\n",
    "#                                \"lr\": lr})\n",
    "#         return AdamW(parameters)\n",
    "\n",
    "    def fetch_scheduler(self):\n",
    "        sch = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            # num_warmup_steps=int(0.1 * self.num_train_steps),\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=self.num_train_steps,\n",
    "            num_cycles=1,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "        return sch\n",
    "\n",
    "    def loss(self, outputs, targets, attention_mask):\n",
    "        # loss_fct = nn.CrossEntropyLoss()\n",
    "        loss_fct = DiceLoss(smooth = 1, square_denominator = True, with_logits = True,  alpha = 0.01 , index_label_position=True)\n",
    "        # loss_fct = DiceLoss(smooth = 1, ohem_ratio = 0.511, square_denominator = True, with_logits = True,  alpha = 0.01 , index_label_position=True)\n",
    "        active_loss = attention_mask.view(-1) == 1\n",
    "        active_logits = outputs.view(-1, self.num_labels)\n",
    "        true_labels = targets.view(-1)\n",
    "        outputs = active_logits.argmax(dim=-1)\n",
    "        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\n",
    "        active_logits = active_logits[idxs]\n",
    "        true_labels = true_labels[idxs].to(torch.long)\n",
    "\n",
    "        loss = loss_fct(active_logits, true_labels)\n",
    "        return loss\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets, attention_mask):\n",
    "        active_loss = (attention_mask.view(-1) == 1).cpu().numpy()\n",
    "        active_logits = outputs.view(-1, self.num_labels)\n",
    "        true_labels = targets.view(-1).cpu().numpy()\n",
    "        outputs = active_logits.argmax(dim=-1).cpu().numpy()\n",
    "        idxs = np.where(active_loss == 1)[0]\n",
    "        f1_score = metrics.f1_score(true_labels[idxs], outputs[idxs], average=\"macro\")\n",
    "        return {\"f1\": f1_score}\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.transformer(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        # all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        # weighted_pooling_embeddings = self.layer_norm(self.pooling(all_hidden_states))\n",
    "        # sequence_output = self.dropout(weighted_pooling_embeddings)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        loss = 0\n",
    "\n",
    "        if targets is not None:\n",
    "            loss1 = self.loss(logits1, targets, attention_mask=mask)\n",
    "            loss2 = self.loss(logits2, targets, attention_mask=mask)\n",
    "            loss3 = self.loss(logits3, targets, attention_mask=mask)\n",
    "            loss4 = self.loss(logits4, targets, attention_mask=mask)\n",
    "            loss5 = self.loss(logits5, targets, attention_mask=mask)\n",
    "            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "            f1_1 = self.monitor_metrics(logits1, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_2 = self.monitor_metrics(logits2, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_3 = self.monitor_metrics(logits3, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_4 = self.monitor_metrics(logits4, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1_5 = self.monitor_metrics(logits5, targets, attention_mask=mask)[\"f1\"]\n",
    "            f1 = (f1_1 + f1_2 + f1_3 + f1_4 + f1_5) / 5\n",
    "            metric = {\"f1\": f1}\n",
    "            return logits, loss, metric\n",
    "\n",
    "        return logits, loss, {}\n",
    "def load_pre(model,pre_model):\n",
    "    model_dict = model.state_dict()\n",
    "    pre_dict = torch.load(pre_model, map_location=torch.device('cuda'))\n",
    "    pre_dict = {k: v for k, v in pre_dict.items() if k in model_dict} \n",
    "    model_dict.update(pre_dict)\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    fold = 5\n",
    "    model = 'microsoft/deberta-base'\n",
    "    lr = 2e-5\n",
    "    output = './autodl-tmp/mdb'\n",
    "    input = './feedback-prize-2021'\n",
    "    max_len = 1536\n",
    "    batch_size = 4\n",
    "    valid_batch_size = 1\n",
    "    epochs = 10\n",
    "    accumulation_steps = 1\n",
    "    eval_schedule = [(float('inf'), 6000), (0.332, 320), (0, 0)]\n",
    "    # eval_schedule = [(float('inf'), 6000), (0.35, 320),(0.34, 160),(0.335, 80), (0.33, 40),(0.325, 20),(0.322, 2), (0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b377316a6cc4766bc1484faa63ea287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0215757ac654552a4876f9a7cc7255f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c63bbfb99e4d9788021c5ae3aa1976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc478f5faa6d4f8a801e542cbb917dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1040 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]3.23it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]4.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]6.22it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "  7%|▋         | 77/1040 [00:01<00:13, 73.98it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "  7%|▋         | 69/1040 [00:00<00:10, 88.68it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      " 10%|▉         | 101/1040 [00:01<00:09, 95.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]1.41it/s]]Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]1.16it/s]]Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1039 [00:00<?, ?it/s]78.77it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1039 [00:00<?, ?it/s]69.27it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 3/1039 [00:00<01:35, 10.89it/s]s]Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1040/1040 [00:11<00:00, 89.09it/s] \n",
      "100%|██████████| 1040/1040 [00:12<00:00, 82.40it/s]\n",
      "100%|██████████| 1040/1040 [00:12<00:00, 83.63it/s]\n",
      "100%|██████████| 1040/1040 [00:12<00:00, 84.23it/s]\n",
      "100%|██████████| 1040/1040 [00:11<00:00, 87.10it/s]\n",
      "100%|██████████| 1040/1040 [00:13<00:00, 76.74it/s]\n",
      "100%|██████████| 1040/1040 [00:12<00:00, 80.46it/s]\n",
      "100%|██████████| 1040/1040 [00:12<00:00, 80.84it/s]\n",
      "100%|██████████| 1040/1040 [00:14<00:00, 70.47it/s]\n",
      "100%|██████████| 1039/1039 [00:12<00:00, 80.89it/s]\n",
      "100%|██████████| 1039/1039 [00:15<00:00, 66.43it/s]\n",
      "100%|██████████| 1039/1039 [01:43<00:00, 10.07it/s]\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]51.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s].29it/s]s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]85.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "  8%|▊         | 22/260 [00:00<00:02, 97.35it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]49.72it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 95/260 [00:00<00:00, 173.83it/s]]Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]177.83it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]169.63it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      " 30%|███       | 78/260 [00:00<00:01, 175.73it/s]]Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "  6%|▌         | 16/259 [00:00<00:02, 117.52it/s]]Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/259 [00:00<?, ?it/s]178.78it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 260/260 [00:01<00:00, 138.00it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 156.20it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 145.15it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 164.59it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 169.37it/s]\n",
      "100%|██████████| 260/260 [00:02<00:00, 117.31it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 165.75it/s]\n",
      "100%|██████████| 260/260 [00:01<00:00, 132.63it/s]\n",
      "100%|██████████| 260/260 [00:02<00:00, 127.76it/s]\n",
      "100%|██████████| 259/259 [00:01<00:00, 132.02it/s]\n",
      "100%|██████████| 259/259 [00:02<00:00, 124.33it/s]\n",
      "100%|██████████| 259/259 [00:02<00:00, 107.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kirob/feedback/runs/1dnfdwui\" target=\"_blank\">dbb-raw-4bs-1dice-fold-0</a></strong> to <a href=\"https://wandb.ai/kirob/feedback\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfe3723348d449283c24a8849739822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99af609d6da64adf99fea6840b0800f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_JOBS = 12\n",
    "args = Config()\n",
    "seed_everything(42)\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "df = pd.read_csv(\"./feedback-prize-2021/train_folds.csv\")\n",
    "for fold in [0]:\n",
    "    train_df = df[df[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    valid_df = df[df[\"kfold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    training_samples = prepare_training_data(train_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "    valid_samples = prepare_training_data(valid_df, tokenizer, args, num_jobs=NUM_JOBS)\n",
    "\n",
    "    train_dataset = FeedbackDataset(training_samples, args.max_len, tokenizer)\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / args.batch_size / args.accumulation_steps * args.epochs)\n",
    "    print(num_train_steps)\n",
    "    evaluation_scheduler = EvaluationScheduler(args.eval_schedule)\n",
    "\n",
    "    run = wandb.init(project='feedback', \n",
    "                         config=args,\n",
    "                         job_type='Test',\n",
    "                         entity=\"kirob\",\n",
    "                         group='dhw',\n",
    "                         tags=['longformer'],\n",
    "                         name=f'dbb-raw-4bs-1dice-fold-{fold}')\n",
    "\n",
    "    model = FeedbackModel(\n",
    "        model_name=args.model,\n",
    "        num_train_steps=num_train_steps,\n",
    "        learning_rate=args.lr,\n",
    "        num_labels=len(target_id_map) - 1,\n",
    "        steps_per_epoch=len(train_dataset) / args.batch_size,\n",
    "    )\n",
    "    # model.load('./autodl-tmp/mdb/650model_2.bin',True)\n",
    "    # model.load_state_dict(load_pre(model,'./autodl-tmp/mdb/model_0.bin'))\n",
    "\n",
    "    vs = Valid_schedule(\n",
    "        model_path=os.path.join(args.output, f\"model_{fold}.bin\"),\n",
    "        valid_df=valid_df,\n",
    "        valid_samples=valid_samples,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        patience=3,\n",
    "        mode=\"max\",\n",
    "        delta=0.001,\n",
    "        save_weights_only=True,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluation_scheduler = evaluation_scheduler,\n",
    "    )\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        train_bs=args.batch_size,\n",
    "        device=\"cuda\",\n",
    "        epochs=args.epochs,\n",
    "        callbacks=[vs],\n",
    "        fp16=True,\n",
    "        accumulation_steps=args.accumulation_steps,\n",
    "        do_adv = False,\n",
    "        adv_emb_name='word_embeddings',\n",
    "        adv_epsilon=1.0,\n",
    "    )\n",
    "    run.finish()\n",
    "# vs.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
